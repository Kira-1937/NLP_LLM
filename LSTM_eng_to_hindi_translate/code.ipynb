{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6c08ee3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "268bf531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"cfilt/iitb-english-hindi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e07ce607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7131d914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_train_file = open(\"source_train.txt\", \"w+\", encoding='utf8')\n",
    "# target_train_file = open(\"target_train.txt\", \"w+\", encoding='utf8')\n",
    "# for translation_pair in dataset[\"train\"][\"translation\"]:\n",
    "#   source_sentence = translation_pair[\"en\"]\n",
    "#   target_sentence = translation_pair[\"hi\"]\n",
    "#   source_train_file.write(source_sentence.strip(\"\\n\") + \"\\n\")\n",
    "#   target_train_file.write(target_sentence.strip(\"\\n\") + \"\\n\")\n",
    "# source_train_file.close()\n",
    "# target_train_file.close()\n",
    "\n",
    "# source_valid_file = open(\"source_valid.txt\", \"w+\", encoding='utf8')\n",
    "# target_valid_file = open(\"target_valid.txt\", \"w+\", encoding='utf8')\n",
    "# for translation_pair in dataset[\"validation\"][\"translation\"]:\n",
    "#   source_sentence = translation_pair[\"en\"]\n",
    "#   target_sentence = translation_pair[\"hi\"]\n",
    "#   source_valid_file.write(source_sentence.strip(\"\\n\") + \"\\n\")\n",
    "#   target_valid_file.write(target_sentence.strip(\"\\n\") + \"\\n\")\n",
    "# source_valid_file.close()\n",
    "# target_valid_file.close()\n",
    "\n",
    "# source_test_file = open(\"source_test.txt\", \"w+\", encoding='utf8')\n",
    "# target_test_file = open(\"target_test.txt\", \"w+\", encoding='utf8')\n",
    "# for translation_pair in dataset[\"test\"][\"translation\"]:\n",
    "#   source_sentence = translation_pair[\"en\"]\n",
    "#   target_sentence = translation_pair[\"hi\"]\n",
    "#   source_test_file.write(source_sentence.strip(\"\\n\") + \"\\n\")\n",
    "#   target_test_file.write(target_sentence.strip(\"\\n\") + \"\\n\")\n",
    "# source_test_file.close()\n",
    "# target_test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5bedafc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          english  \\\n",
      "0  Give your application an accessibility workout   \n",
      "1               Accerciser Accessibility Explorer   \n",
      "2  The default plugin layout for the bottom panel   \n",
      "3     The default plugin layout for the top panel   \n",
      "4  A list of plugins that are disabled by default   \n",
      "\n",
      "                                               hindi  \n",
      "0    अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें  \n",
      "1                    एक्सेर्साइसर पहुंचनीयता अन्वेषक  \n",
      "2              निचले पटल के लिए डिफोल्ट प्लग-इन खाका  \n",
      "3               ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका  \n",
      "4  उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से नि...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load English and Hindi sentences\n",
    "with open(\"source_train.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    en_lines = [line.strip() for line in f.readlines()]\n",
    "\n",
    "with open(\"target_train.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hi_lines = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\"english\": en_lines, \"hindi\": hi_lines})\n",
    "\n",
    "# Preview\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0bdef98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             english  \\\n",
      "0                           A black box in your car?   \n",
      "1  As America's road planners struggle to find th...   \n",
      "2  The devices, which track every mile a motorist...   \n",
      "3  The usually dull arena of highway planning has...   \n",
      "4  Libertarians have joined environmental groups ...   \n",
      "\n",
      "                                               hindi  \n",
      "0                          आपकी कार में ब्लैक बॉक्स?  \n",
      "1  जबकि अमेरिका के सड़क योजनाकार, ध्वस्त होते हुए...  \n",
      "2  यह डिवाइस, जो मोटर-चालक द्वारा वाहन चलाए गए प्...  \n",
      "3  आम तौर पर हाईवे नियोजन जैसा उबाऊ काम भी अचानक ...  \n",
      "4  आपने द्वारा ड्राइव किए गए मील, तथा संभवतः ड्रा...  \n"
     ]
    }
   ],
   "source": [
    "# FOR TESTING\n",
    "# Load English and Hindi sentences\n",
    "with open(\"source_test.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    en_lines = [line.strip() for line in f.readlines()]\n",
    "\n",
    "with open(\"target_test.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hi_lines = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Create DataFrame\n",
    "test_df = pd.DataFrame({\"english\": en_lines, \"hindi\": hi_lines})\n",
    "\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "04b1fd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Backup full datasets\n",
    "full_df = df.copy()\n",
    "full_test_df = test_df.copy()\n",
    "\n",
    "# Sample 10% of the data\n",
    "df = df.sample(frac=0.001, random_state=42).reset_index(drop=True)\n",
    "test_df = test_df.sample(frac=0.001, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3d6687f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>en_tokens</th>\n",
       "      <th>hindi</th>\n",
       "      <th>hi_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rodent</td>\n",
       "      <td>[rodent]</td>\n",
       "      <td>कृंतक प्राणी</td>\n",
       "      <td>[कृंतक, प्राणी]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IDPL Chairman Major - General - LRB - retd - R...</td>\n",
       "      <td>[idpl, chairman, major, -, general, -, lrb, -,...</td>\n",
       "      <td>आइड़ीपीएल के अध्यक्ष मेजर-जनरल (सेवानिवृत्त) व...</td>\n",
       "      <td>[आइड़ीपीएल, के, अध्यक्ष, मेजर-जनरल, (सेवानिवृत...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We needed something to hold the net up</td>\n",
       "      <td>[we, needed, something, to, hold, the, net, up]</td>\n",
       "      <td>हमें जाल को ऊपर रखे रहने के लिए कुछ चाहिए था</td>\n",
       "      <td>[हमें, जाल, को, ऊपर, रखे, रहने, के, लिए, कुछ, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Believers, on Friday when the call for prayer ...</td>\n",
       "      <td>[believers,, on, friday, when, the, call, for,...</td>\n",
       "      <td>ऐ ईमान लानेवालो, जब जुमा के दिन नमाज़ के लिए प...</td>\n",
       "      <td>[ऐ, ईमान, लानेवालो,, जब, जुमा, के, दिन, नमाज़,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Explanation.—A trust which is not declared by...</td>\n",
       "      <td>[[explanation.—a, trust, which, is, not, decla...</td>\n",
       "      <td>स्पष्टीकरण–किसी न्यास को, जो सम्यक् रूप से निष...</td>\n",
       "      <td>[स्पष्टीकरण–किसी, न्यास, को,, जो, सम्यक्, रूप,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0                                             rodent   \n",
       "1  IDPL Chairman Major - General - LRB - retd - R...   \n",
       "2             We needed something to hold the net up   \n",
       "3  Believers, on Friday when the call for prayer ...   \n",
       "4  [Explanation.—A trust which is not declared by...   \n",
       "\n",
       "                                           en_tokens  \\\n",
       "0                                           [rodent]   \n",
       "1  [idpl, chairman, major, -, general, -, lrb, -,...   \n",
       "2    [we, needed, something, to, hold, the, net, up]   \n",
       "3  [believers,, on, friday, when, the, call, for,...   \n",
       "4  [[explanation.—a, trust, which, is, not, decla...   \n",
       "\n",
       "                                               hindi  \\\n",
       "0                                       कृंतक प्राणी   \n",
       "1  आइड़ीपीएल के अध्यक्ष मेजर-जनरल (सेवानिवृत्त) व...   \n",
       "2       हमें जाल को ऊपर रखे रहने के लिए कुछ चाहिए था   \n",
       "3  ऐ ईमान लानेवालो, जब जुमा के दिन नमाज़ के लिए प...   \n",
       "4  स्पष्टीकरण–किसी न्यास को, जो सम्यक् रूप से निष...   \n",
       "\n",
       "                                           hi_tokens  \n",
       "0                                    [कृंतक, प्राणी]  \n",
       "1  [आइड़ीपीएल, के, अध्यक्ष, मेजर-जनरल, (सेवानिवृत...  \n",
       "2  [हमें, जाल, को, ऊपर, रखे, रहने, के, लिए, कुछ, ...  \n",
       "3  [ऐ, ईमान, लानेवालो,, जब, जुमा, के, दिन, नमाज़,...  \n",
       "4  [स्पष्टीकरण–किसी, न्यास, को,, जो, सम्यक्, रूप,...  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic word-level tokenizer for English and Hindi\n",
    "def tokenize(sentence):\n",
    "    return sentence.lower().split()\n",
    "\n",
    "# Apply tokenization\n",
    "df['en_tokens'] = df['english'].apply(tokenize)\n",
    "df['hi_tokens'] = df['hindi'].apply(tokenize)\n",
    "\n",
    "# View result\n",
    "df[['english', 'en_tokens', 'hindi', 'hi_tokens']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f18a64e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>en_tokens</th>\n",
       "      <th>hindi</th>\n",
       "      <th>hi_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Due to financial constraints Dhirubhai had to ...</td>\n",
       "      <td>[due, to, financial, constraints, dhirubhai, h...</td>\n",
       "      <td>आर्थिक तंगी के कारण धीरूभाई को हाईस्कूल के बाद...</td>\n",
       "      <td>[आर्थिक, तंगी, के, कारण, धीरूभाई, को, हाईस्कूल...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Or they can choose not to have a device at all...</td>\n",
       "      <td>[or, they, can, choose, not, to, have, a, devi...</td>\n",
       "      <td>या फिर से कोई भी डिवाइस न लेना चुन सकते हैं, ब...</td>\n",
       "      <td>[या, फिर, से, कोई, भी, डिवाइस, न, लेना, चुन, स...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>However, following the recent murder of Austra...</td>\n",
       "      <td>[however,, following, the, recent, murder, of,...</td>\n",
       "      <td>वैसे हाल ही में फुकेट में ऑस्ट्रेलियाई ट्रैवेल...</td>\n",
       "      <td>[वैसे, हाल, ही, में, फुकेट, में, ऑस्ट्रेलियाई,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0  Due to financial constraints Dhirubhai had to ...   \n",
       "1  Or they can choose not to have a device at all...   \n",
       "2  However, following the recent murder of Austra...   \n",
       "\n",
       "                                           en_tokens  \\\n",
       "0  [due, to, financial, constraints, dhirubhai, h...   \n",
       "1  [or, they, can, choose, not, to, have, a, devi...   \n",
       "2  [however,, following, the, recent, murder, of,...   \n",
       "\n",
       "                                               hindi  \\\n",
       "0  आर्थिक तंगी के कारण धीरूभाई को हाईस्कूल के बाद...   \n",
       "1  या फिर से कोई भी डिवाइस न लेना चुन सकते हैं, ब...   \n",
       "2  वैसे हाल ही में फुकेट में ऑस्ट्रेलियाई ट्रैवेल...   \n",
       "\n",
       "                                           hi_tokens  \n",
       "0  [आर्थिक, तंगी, के, कारण, धीरूभाई, को, हाईस्कूल...  \n",
       "1  [या, फिर, से, कोई, भी, डिवाइस, न, लेना, चुन, स...  \n",
       "2  [वैसे, हाल, ही, में, फुकेट, में, ऑस्ट्रेलियाई,...  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic word-level tokenizer for English and Hindi\n",
    "\n",
    "\n",
    "# Apply tokenization\n",
    "test_df['en_tokens'] = test_df['english'].apply(tokenize)\n",
    "test_df['hi_tokens'] = test_df['hindi'].apply(tokenize)\n",
    "\n",
    "# View result\n",
    "test_df[['english', 'en_tokens', 'hindi', 'hi_tokens']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "15abdfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(token_lists, min_freq=1):\n",
    "    counter = Counter(token for tokens in token_lists for token in tokens)\n",
    "    vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
    "    for word, freq in counter.items():\n",
    "        if freq >= min_freq and word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "aeb884c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocab size: 7167\n",
      "Hindi vocab size: 7264\n"
     ]
    }
   ],
   "source": [
    "# Build vocabularies\n",
    "en_vocab = build_vocab(df['en_tokens'])\n",
    "hi_vocab = build_vocab(df['hi_tokens'])\n",
    "\n",
    "print(\"English vocab size:\", len(en_vocab))\n",
    "print(\"Hindi vocab size:\", len(hi_vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9af9bc59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>en_indices</th>\n",
       "      <th>hindi</th>\n",
       "      <th>hi_indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rodent</td>\n",
       "      <td>[1, 4, 2]</td>\n",
       "      <td>कृंतक प्राणी</td>\n",
       "      <td>[1, 4, 5, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IDPL Chairman Major - General - LRB - retd - R...</td>\n",
       "      <td>[1, 5, 6, 7, 8, 9, 8, 10, 8, 11, 8, 12, 8, 13,...</td>\n",
       "      <td>आइड़ीपीएल के अध्यक्ष मेजर-जनरल (सेवानिवृत्त) व...</td>\n",
       "      <td>[1, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We needed something to hold the net up</td>\n",
       "      <td>[1, 40, 41, 42, 28, 43, 20, 44, 30, 2]</td>\n",
       "      <td>हमें जाल को ऊपर रखे रहने के लिए कुछ चाहिए था</td>\n",
       "      <td>[1, 42, 43, 14, 44, 45, 46, 7, 32, 47, 48, 49, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Believers, on Friday when the call for prayer ...</td>\n",
       "      <td>[1, 45, 46, 47, 48, 20, 49, 34, 50, 51, 52, 53...</td>\n",
       "      <td>ऐ ईमान लानेवालो, जब जुमा के दिन नमाज़ के लिए प...</td>\n",
       "      <td>[1, 50, 51, 52, 53, 54, 7, 55, 56, 7, 32, 57, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Explanation.—A trust which is not declared by...</td>\n",
       "      <td>[1, 70, 71, 72, 51, 73, 74, 75, 76, 77, 78, 79...</td>\n",
       "      <td>स्पष्टीकरण–किसी न्यास को, जो सम्यक् रूप से निष...</td>\n",
       "      <td>[1, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0                                             rodent   \n",
       "1  IDPL Chairman Major - General - LRB - retd - R...   \n",
       "2             We needed something to hold the net up   \n",
       "3  Believers, on Friday when the call for prayer ...   \n",
       "4  [Explanation.—A trust which is not declared by...   \n",
       "\n",
       "                                          en_indices  \\\n",
       "0                                          [1, 4, 2]   \n",
       "1  [1, 5, 6, 7, 8, 9, 8, 10, 8, 11, 8, 12, 8, 13,...   \n",
       "2             [1, 40, 41, 42, 28, 43, 20, 44, 30, 2]   \n",
       "3  [1, 45, 46, 47, 48, 20, 49, 34, 50, 51, 52, 53...   \n",
       "4  [1, 70, 71, 72, 51, 73, 74, 75, 76, 77, 78, 79...   \n",
       "\n",
       "                                               hindi  \\\n",
       "0                                       कृंतक प्राणी   \n",
       "1  आइड़ीपीएल के अध्यक्ष मेजर-जनरल (सेवानिवृत्त) व...   \n",
       "2       हमें जाल को ऊपर रखे रहने के लिए कुछ चाहिए था   \n",
       "3  ऐ ईमान लानेवालो, जब जुमा के दिन नमाज़ के लिए प...   \n",
       "4  स्पष्टीकरण–किसी न्यास को, जो सम्यक् रूप से निष...   \n",
       "\n",
       "                                          hi_indices  \n",
       "0                                       [1, 4, 5, 2]  \n",
       "1  [1, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17...  \n",
       "2  [1, 42, 43, 14, 44, 45, 46, 7, 32, 47, 48, 49, 2]  \n",
       "3  [1, 50, 51, 52, 53, 54, 7, 55, 56, 7, 32, 57, ...  \n",
       "4  [1, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86...  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokens_to_indices(tokens, vocab, add_sos_eos=True):\n",
    "    indices = [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
    "    if add_sos_eos:\n",
    "        indices = [vocab['<sos>']] + indices + [vocab['<eos>']]\n",
    "    return indices\n",
    "\n",
    "# Convert all tokens to sequences\n",
    "df['en_indices'] = df['en_tokens'].apply(lambda x: tokens_to_indices(x, en_vocab, add_sos_eos=True))\n",
    "df['hi_indices'] = df['hi_tokens'].apply(lambda x: tokens_to_indices(x, hi_vocab, add_sos_eos=True))\n",
    "\n",
    "df[['english', 'en_indices', 'hindi', 'hi_indices']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "365728ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>en_indices</th>\n",
       "      <th>hindi</th>\n",
       "      <th>hi_indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Due to financial constraints Dhirubhai had to ...</td>\n",
       "      <td>[1, 3959, 28, 3599, 3, 3, 790, 28, 3, 97, 192,...</td>\n",
       "      <td>आर्थिक तंगी के कारण धीरूभाई को हाईस्कूल के बाद...</td>\n",
       "      <td>[1, 824, 3011, 7, 734, 3, 14, 3, 7, 132, 143, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Or they can choose not to have a device at all...</td>\n",
       "      <td>[1, 94, 126, 229, 4941, 73, 28, 136, 76, 2577,...</td>\n",
       "      <td>या फिर से कोई भी डिवाइस न लेना चुन सकते हैं, ब...</td>\n",
       "      <td>[1, 103, 1044, 82, 38, 36, 3, 205, 4932, 986, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>However, following the recent murder of Austra...</td>\n",
       "      <td>[1, 1074, 1538, 20, 1739, 3, 84, 1473, 3, 2742...</td>\n",
       "      <td>वैसे हाल ही में फुकेट में ऑस्ट्रेलियाई ट्रैवेल...</td>\n",
       "      <td>[1, 865, 1832, 143, 19, 3, 19, 3, 3, 3, 3, 3, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0  Due to financial constraints Dhirubhai had to ...   \n",
       "1  Or they can choose not to have a device at all...   \n",
       "2  However, following the recent murder of Austra...   \n",
       "\n",
       "                                          en_indices  \\\n",
       "0  [1, 3959, 28, 3599, 3, 3, 790, 28, 3, 97, 192,...   \n",
       "1  [1, 94, 126, 229, 4941, 73, 28, 136, 76, 2577,...   \n",
       "2  [1, 1074, 1538, 20, 1739, 3, 84, 1473, 3, 2742...   \n",
       "\n",
       "                                               hindi  \\\n",
       "0  आर्थिक तंगी के कारण धीरूभाई को हाईस्कूल के बाद...   \n",
       "1  या फिर से कोई भी डिवाइस न लेना चुन सकते हैं, ब...   \n",
       "2  वैसे हाल ही में फुकेट में ऑस्ट्रेलियाई ट्रैवेल...   \n",
       "\n",
       "                                          hi_indices  \n",
       "0  [1, 824, 3011, 7, 734, 3, 14, 3, 7, 132, 143, ...  \n",
       "1  [1, 103, 1044, 82, 38, 36, 3, 205, 4932, 986, ...  \n",
       "2  [1, 865, 1832, 143, 19, 3, 19, 3, 3, 3, 3, 3, ...  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert all tokens to sequences\n",
    "test_df['en_indices'] = test_df['en_tokens'].apply(lambda x: tokens_to_indices(x, en_vocab, add_sos_eos=True))\n",
    "test_df['hi_indices'] = test_df['hi_tokens'].apply(lambda x: tokens_to_indices(x, hi_vocab, add_sos_eos=True))\n",
    "\n",
    "test_df[['english', 'en_indices', 'hindi', 'hi_indices']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a7ecc737",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EN_LEN = 40\n",
    "MAX_HI_LEN = 40\n",
    "\n",
    "df = df[df['en_indices'].apply(len) <= MAX_EN_LEN]\n",
    "df = df[df['hi_indices'].apply(len) <= MAX_HI_LEN]\n",
    "\n",
    "test_df = test_df[test_df['en_indices'].apply(len) <= MAX_EN_LEN]\n",
    "test_df = test_df[test_df['hi_indices'].apply(len) <= MAX_HI_LEN]\n",
    "def pad_sequence(seq, max_len, pad_value=0):\n",
    "    return seq[:max_len] + [pad_value] * max(0, max_len - len(seq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3d32bb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['en_padded'] = df['en_indices'].apply(lambda x: pad_sequence(x, MAX_EN_LEN, pad_value=en_vocab['<pad>']))\n",
    "df['hi_padded'] = df['hi_indices'].apply(lambda x: pad_sequence(x, MAX_HI_LEN, pad_value=hi_vocab['<pad>']))\n",
    "\n",
    "\n",
    "test_df['en_padded'] = test_df['en_indices'].apply(lambda x: pad_sequence(x, MAX_EN_LEN, pad_value=en_vocab['<pad>']))\n",
    "test_df['hi_padded'] = test_df['hi_indices'].apply(lambda x: pad_sequence(x, MAX_HI_LEN, pad_value=hi_vocab['<pad>']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8d262153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1520, 8)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "818bebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset ,DataLoader,Subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b8014cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class translation_dataset(Dataset):\n",
    "   def __init__(self,df):\n",
    "      self.e=df['en_padded'].tolist()\n",
    "      self.h=df['hi_padded'].tolist()\n",
    "  \n",
    "   def __len__(self):\n",
    "      return len(self.e)\n",
    "   \n",
    "   def __getitem__(self,idx):\n",
    "      e=torch.tensor(self.e[idx],dtype=torch.long)\n",
    "      h=torch.tensor(self.h[idx],dtype=torch.long)\n",
    "      return e,h\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset=translation_dataset(df)\n",
    "test_dataset=translation_dataset(test_df)\n",
    "test_dataloader=DataLoader(test_dataset,batch_size=10,shuffle=True)\n",
    "dataloader=DataLoader(dataset,batch_size=10,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "93df8e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English batch shape: torch.Size([10, 40])\n",
      "Hindi batch shape: torch.Size([10, 40])\n",
      "Sample English indices: tensor([   1,   20, 5456, 2238, 5457,   51,  263,  515,   20,  180, 4823,   23,\n",
      "          19,   20, 5458,   23, 5459, 4235,    2,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0])\n",
      "Sample Hindi indices: tensor([   1, 2923,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0])\n"
     ]
    }
   ],
   "source": [
    "# Inspect a batch\n",
    "for en_batch, hi_batch in dataloader:\n",
    "    print(\"English batch shape:\", en_batch.shape)  # (batch_size, seq_len)\n",
    "    print(\"Hindi batch shape:\", hi_batch.shape)    # (batch_size, seq_len)\n",
    "    print(\"Sample English indices:\", en_batch[9])\n",
    "    print(\"Sample Hindi indices:\", hi_batch[0])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "83e47175",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "826dfa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder class\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,input_dim,emb_dim,hidden_dim):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.embedding=nn.Embedding(input_dim,emb_dim)\n",
    "        self.lstm=nn.LSTM(emb_dim,hidden_dim,num_layers=1,batch_first=True)\n",
    "    \n",
    "    def forward(self,x):\n",
    "          \n",
    "        embedded=self.embedding(x)\n",
    "        \n",
    "        output,(short_enc,long_enc)=self.lstm(embedded)\n",
    "\n",
    "        return short_enc,long_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c3f59eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder class\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,output_dim,emb_dim,hidden_dim):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.embedding=nn.Embedding(output_dim,emb_dim)\n",
    "        self.lstm=nn.LSTM(emb_dim,hidden_dim,batch_first=True)\n",
    "        self.fc=nn.Linear(hidden_dim,output_dim)\n",
    "\n",
    "    def forward(self,x,short_enc,long_enc):\n",
    "          \n",
    "        embedded=self.embedding(x)\n",
    "        \n",
    "        if embedded.dim() == 2:\n",
    "            embedded = embedded.unsqueeze(1)  # only if it's [batch_size, embed_dim]\n",
    "        outputs,(short_dec,long_dec)=self.lstm(embedded,(short_enc,long_enc))\n",
    "        prediction=self.fc(outputs.squeeze(1))\n",
    "        return prediction,short_dec,long_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1bfce094",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,encoder,decoder,device):\n",
    "        super(Seq2Seq,self).__init__()\n",
    "        self.encoder=encoder\n",
    "        self.decoder=decoder\n",
    "        self.device=device\n",
    "        \n",
    "\n",
    "    def forward(self,source,target,teacher_forcing_ratio=0.5):\n",
    "        # source: batch of source sequences (e.g. English)\n",
    "        # target: batch of target sequences (e.g. Hindi)\n",
    "        # teacher_forcing_ratio: probability that decoder gets the correct previous token (vs. its own output)\n",
    "        batch_size=source.shape[0]\n",
    "        target_len=target.shape[1]\n",
    "        target_vocab_size=self.decoder.fc.out_features\n",
    "\n",
    "        outputs=torch.zeros(batch_size,target_len,target_vocab_size)\n",
    "        \n",
    "        short_enc,long_enc=self.encoder(source)\n",
    "        \n",
    "        input=target[:,0]\n",
    "\n",
    "        for t in range(1,target_len):\n",
    "             output,short_dec,long_dec=self.decoder(input,short_enc,long_enc)\n",
    "             \n",
    "             outputs[:, t] = output\n",
    "             \n",
    "             teacher_force=torch.rand(1).item()<teacher_forcing_ratio\n",
    "             top1=output.argmax(1)\n",
    "\n",
    "             input=target[:,t] if teacher_force else top1\n",
    "        \n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "067b451f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7167, 256)\n",
       "    (lstm): LSTM(256, 512, batch_first=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(7264, 256)\n",
       "    (lstm): LSTM(256, 512, batch_first=True)\n",
       "    (fc): Linear(in_features=512, out_features=7264, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Initialize Everything\n",
    "input_dim=len(en_vocab)\n",
    "output_dim=len(hi_vocab)\n",
    "\n",
    "emb_dim=256\n",
    "hidden_dim=512\n",
    "\n",
    "Device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "enc=Encoder(input_dim,emb_dim,hidden_dim)\n",
    "\n",
    "dec=Decoder(output_dim,emb_dim,hidden_dim)\n",
    "\n",
    "model=Seq2Seq(enc,dec,Device).to(Device)\n",
    "model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "641dc512",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss(ignore_index=hi_vocab['<pad>'])\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f71578b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "    model.train()\n",
    "    epoch_loss=0.0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        source=batch[0].to(Device)\n",
    "        target=batch[1].to(Device)\n",
    "\n",
    "        optimizer.zero_grad() #clear gradient\n",
    "\n",
    "        output=model(source,target,teacher_forcing_ratio=0.5)\n",
    "\n",
    "        # Reshape output and target to 2D for loss computation\n",
    "        # Output: [batch_size, trg_len, vocab_size] → [batch_size * trg_len, vocab_size]\n",
    "        # Target: [batch_size, trg_len] → [batch_size * trg_len]\n",
    "\n",
    "        output = output[:, 1:].reshape(-1, output.shape[-1])\n",
    "        # output = output.squeeze(1)  # Shape becomes [batch_size, vocab_size]\n",
    "        target = target[:, 1:].reshape(-1)\n",
    "\n",
    "        loss=criterion(output,target)\n",
    "        loss.backward()  #backpropagation\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)  # Prevent exploding gradients\n",
    "\n",
    "        optimizer.step()\n",
    "        epoch_loss+=loss.item()\n",
    "    \n",
    "\n",
    "    return epoch_loss/len(dataloader)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "351a9e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_one_epoch():\n",
    "    model.eval()\n",
    "    epoch_loss=0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            src = batch[0].to(Device)\n",
    "            trg = batch[1].to(Device)\n",
    "\n",
    "            output = model(src, trg, 0)  # No teacher forcing\n",
    "\n",
    "            output = output[:, 1:].reshape(-1, output.shape[-1])\n",
    "            # output = output.squeeze(1)  # Shape becomes [batch_size, vocab_size]\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903c4aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\tTrain Loss: 6.7073\n",
      "\tTest Loss: 7.9519\n",
      "Epoch 2\n",
      "\tTrain Loss: 6.5109\n",
      "\tTest Loss: 8.1501\n",
      "Epoch 3\n",
      "\tTrain Loss: 6.2870\n",
      "\tTest Loss: 8.1732\n",
      "Epoch 4\n",
      "\tTrain Loss: 6.0607\n",
      "\tTest Loss: 8.3495\n",
      "Epoch 5\n",
      "\tTrain Loss: 5.8527\n",
      "\tTest Loss: 8.3152\n",
      "Epoch 6\n",
      "\tTrain Loss: 5.6349\n",
      "\tTest Loss: 8.4368\n",
      "Epoch 7\n",
      "\tTrain Loss: 5.3501\n",
      "\tTest Loss: 8.6487\n",
      "Epoch 8\n",
      "\tTrain Loss: 5.1132\n",
      "\tTest Loss: 8.7360\n",
      "Epoch 9\n",
      "\tTrain Loss: 4.8281\n",
      "\tTest Loss: 8.7683\n",
      "Epoch 10\n",
      "\tTrain Loss: 4.5966\n",
      "\tTest Loss: 8.4347\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train_one_epoch()\n",
    "    test_loss=evaluation_one_epoch()\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:.4f}\")\n",
    "    print(f\"\\tTest Loss: {test_loss:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "94a38a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"seq2seq_translation.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3219b2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence, src_vocab, trg_vocab, device, max_len=50):\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize input sentence\n",
    "    tokens = sentence.lower().strip().split()\n",
    "    tokens = ['<sos>'] + tokens + ['<eos>']\n",
    "    print(\"tokens=\",tokens)\n",
    "    \n",
    "    # Numericalize\n",
    "    src_indexes = [src_vocab.get(token, src_vocab['<unk>']) for token in tokens]\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)  # shape: [1, seq_len]\n",
    "\n",
    "    # Encode\n",
    "    with torch.no_grad():\n",
    "        short_enc, long_enc = model.encoder(src_tensor)\n",
    "\n",
    "    # Decoder starts with <sos>\n",
    "    trg_indexes = [trg_vocab['<sos>']]\n",
    "    input_token = torch.tensor([trg_vocab['<sos>']], device=device)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            output, short_enc, long_enc = model.decoder(input_token, short_enc, long_enc)  # output shape: [1, 1, vocab_size] or [1, vocab_size]\n",
    "\n",
    "        # Make sure to match the output shape\n",
    "        if output.dim() == 3:\n",
    "            pred_token = output.argmax(2).item()  # if shape [1, 1, vocab_size]\n",
    "        else:\n",
    "            pred_token = output.argmax(1).item()  # if shape [1, vocab_size]\n",
    "\n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == trg_vocab['<eos>']:\n",
    "            break\n",
    "\n",
    "        input_token = torch.tensor([pred_token], device=device)\n",
    "\n",
    "    # Convert token ids to words\n",
    "    inv_trg_vocab = {i: w for w, i in trg_vocab.items()}\n",
    "    # print(\"inv_trg_vocab\",inv_trg_vocab)\n",
    "    trg_tokens = [inv_trg_vocab[i] for i in trg_indexes[1:-1]]  # skip <sos> and <eos>\n",
    "    print(\"trg_tokens\",trg_tokens)\n",
    "    return ' '.join(trg_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "eb353707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens= ['<sos>', 'how', 'are', 'you', '<eos>']\n",
      "trg_tokens ['बलराम', 'हार्वर्ड-क्योटो', 'में']\n",
      "Translation: बलराम हार्वर्ड-क्योटो में\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"how are you\"\n",
    "translation = translate_sentence(model, test_sentence, en_vocab, hi_vocab, device=Device)\n",
    "print(\"Translation:\", translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a13dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9afb19b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61264549",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
